{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AFML Part 1 - Team 44_XLR8 (v2 - OPTIMIZED)\n",
        "## Multiple Proven Techniques for NMSE < 0.3\n",
        "\n",
        "**New Strategy**:\n",
        "1. Ensemble of 3 models\n",
        "2. Better architecture\n",
        "3. Data augmentation\n",
        "4. Weighted loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "    print(\"✅ Using M2 GPU\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(\"✅ Using CUDA GPU\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"⚠️  Using CPU\")\n",
        "\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading...\")\n",
        "train_clean = pd.read_csv('train-part1-clean.csv').values.astype(np.float32)\n",
        "train_noise = pd.read_csv('train-part1-noise.csv').values.astype(np.float32)\n",
        "test_data = pd.read_csv('test-part1.csv').values.astype(np.float32)\n",
        "\n",
        "print(f\"Clean: {train_clean.shape}\")\n",
        "print(f\"Noisy: {train_noise.shape}\")\n",
        "print(f\"Test: {test_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing - Normalize by Column Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    train_noise, train_clean, test_size=0.15, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}\")\n",
        "\n",
        "# Normalize by column (each feature separately)\n",
        "X_mean = X_train.mean(axis=0, keepdims=True)\n",
        "X_std = X_train.std(axis=0, keepdims=True) + 1e-8\n",
        "\n",
        "y_mean = y_train.mean(axis=0, keepdims=True)\n",
        "y_std = y_train.std(axis=0, keepdims=True) + 1e-8\n",
        "\n",
        "X_train_norm = (X_train - X_mean) / X_std\n",
        "X_val_norm = (X_val - X_mean) / X_std\n",
        "test_norm = (test_data - X_mean) / X_std\n",
        "\n",
        "y_train_norm = (y_train - y_mean) / y_std\n",
        "y_val_norm = (y_val - y_mean) / y_std\n",
        "\n",
        "# To tensors\n",
        "X_train_t = torch.FloatTensor(X_train_norm)\n",
        "y_train_t = torch.FloatTensor(y_train_norm)\n",
        "X_val_t = torch.FloatTensor(X_val_norm)\n",
        "y_val_t = torch.FloatTensor(y_val_norm)\n",
        "test_t = torch.FloatTensor(test_norm)\n",
        "\n",
        "print(\"✓ Normalized by column statistics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Improved Model with Residual Connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.LayerNorm(dim)\n",
        "        )\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.relu(x + self.block(x))\n",
        "\n",
        "class ImprovedDenoiser(nn.Module):\n",
        "    def __init__(self, input_dim=20):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        \n",
        "        self.res_blocks = nn.Sequential(\n",
        "            ResidualBlock(512),\n",
        "            ResidualBlock(512),\n",
        "            ResidualBlock(512),\n",
        "            ResidualBlock(512)\n",
        "        )\n",
        "        \n",
        "        self.output_proj = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, input_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.res_blocks(x)\n",
        "        x = self.output_proj(x)\n",
        "        return x\n",
        "\n",
        "print(\"Creating ensemble of 3 models...\")\n",
        "models = [ImprovedDenoiser().to(device) for _ in range(3)]\n",
        "print(f\"Params per model: {sum(p.numel() for p in models[0].parameters()):,}\")\n",
        "print(f\"Total params: {sum(p.numel() for p in models[0].parameters()) * 3:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 2048\n",
        "NUM_EPOCHS = 80\n",
        "LR = 0.001\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    TensorDataset(X_train_t, y_train_t),\n",
        "    batch_size=BATCH_SIZE, shuffle=True, drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    TensorDataset(X_val_t, y_val_t),\n",
        "    batch_size=BATCH_SIZE, shuffle=False\n",
        ")\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizers = [optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5) for model in models]\n",
        "schedulers = [optim.lr_scheduler.CosineAnnealingLR(opt, T_max=NUM_EPOCHS, eta_min=1e-6) for opt in optimizers]\n",
        "\n",
        "print(f\"Batches/epoch: {len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_val_losses = [float('inf')] * 3\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Train all models\n",
        "    for model in models:\n",
        "        model.train()\n",
        "    \n",
        "    train_losses = [0, 0, 0]\n",
        "    \n",
        "    for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        \n",
        "        for i, (model, optimizer) in enumerate(zip(models, optimizers)):\n",
        "            pred = model(X_batch)\n",
        "            loss = criterion(pred, y_batch)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_losses[i] += loss.item()\n",
        "    \n",
        "    train_losses = [tl / len(train_loader) for tl in train_losses]\n",
        "    \n",
        "    # Validate\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "    \n",
        "    val_losses = [0, 0, 0]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            \n",
        "            for i, model in enumerate(models):\n",
        "                pred = model(X_batch)\n",
        "                loss = criterion(pred, y_batch)\n",
        "                val_losses[i] += loss.item()\n",
        "    \n",
        "    val_losses = [vl / len(val_loader) for vl in val_losses]\n",
        "    \n",
        "    # Update schedulers\n",
        "    for scheduler in schedulers:\n",
        "        scheduler.step()\n",
        "    \n",
        "    # Save best models\n",
        "    saved = []\n",
        "    for i, (model, val_loss) in enumerate(zip(models, val_losses)):\n",
        "        if val_loss < best_val_losses[i]:\n",
        "            best_val_losses[i] = val_loss\n",
        "            torch.save(model.state_dict(), f'model_{i}.pth')\n",
        "            saved.append(i)\n",
        "    \n",
        "    avg_train = np.mean(train_losses)\n",
        "    avg_val = np.mean(val_losses)\n",
        "    \n",
        "    if saved or (epoch+1) % 10 == 0:\n",
        "        saved_str = f\" [SAVED: {saved}]\" if saved else \"\"\n",
        "        print(f\"Epoch {epoch+1} - Train: {avg_train:.6f}, Val: {avg_val:.6f}{saved_str}\")\n",
        "\n",
        "print(f\"\\nBest val losses: {[f'{v:.6f}' for v in best_val_losses]}\")\n",
        "print(f\"Average: {np.mean(best_val_losses):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ensemble Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best models\n",
        "for i, model in enumerate(models):\n",
        "    model.load_state_dict(torch.load(f'model_{i}.pth'))\n",
        "    model.eval()\n",
        "\n",
        "print(\"Predicting with ensemble...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_t_device = test_t.to(device)\n",
        "    X_val_t_device = X_val_t.to(device)\n",
        "    \n",
        "    # Ensemble predictions\n",
        "    test_preds = [model(test_t_device).cpu().numpy() for model in models]\n",
        "    val_preds = [model(X_val_t_device).cpu().numpy() for model in models]\n",
        "    \n",
        "    # Average\n",
        "    test_pred_norm = np.mean(test_preds, axis=0)\n",
        "    val_pred_norm = np.mean(val_preds, axis=0)\n",
        "\n",
        "# Denormalize\n",
        "test_pred = test_pred_norm * y_std + y_mean\n",
        "val_pred = val_pred_norm * y_std + y_mean\n",
        "\n",
        "print(\"✓ Ensemble prediction done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate NMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mse = np.mean((y_val - val_pred) ** 2)\n",
        "variance = np.var(y_val)\n",
        "nmse = mse / variance\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"VALIDATION NMSE: {nmse:.6f}\")\n",
        "print(f\"Target: < 0.3\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "if nmse < 0.3:\n",
        "    print(f\"✅ SUCCESS! NMSE < 0.3!\")\n",
        "elif nmse < 0.5:\n",
        "    print(f\"⚠️  Close! NMSE = {nmse:.4f}\")\n",
        "else:\n",
        "    print(f\"❌ NMSE too high: {nmse:.4f}\")\n",
        "\n",
        "print(f\"\\nMSE: {mse:.8f}\")\n",
        "print(f\"Variance: {variance:.8f}\")\n",
        "print(f\"Avg Val Loss: {np.mean(best_val_losses):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission = pd.DataFrame(test_pred)\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"\\n✓ Saved: submission.csv\")\n",
        "print(f\"Shape: {submission.shape}\")\n",
        "submission.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "for i in range(20):\n",
        "    plt.scatter(y_val[:1000, i], val_pred[:1000, i], alpha=0.3, s=1)\n",
        "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title(f'All Features (NMSE={nmse:.4f})')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "errors = np.abs(y_val - val_pred)\n",
        "plt.hist(errors.flatten(), bins=100, alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Absolute Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Error Distribution')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal NMSE: {nmse:.6f}\")\n",
        "print(f\"Mean Absolute Error: {np.mean(errors):.6f}\")\n",
        "print(f\"Median Absolute Error: {np.median(errors):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**Improvements in v2:**\n",
        "1. ✅ Ensemble of 3 models (reduces variance)\n",
        "2. ✅ Residual blocks (better gradient flow)\n",
        "3. ✅ LayerNorm (more stable than BatchNorm)\n",
        "4. ✅ Column-wise normalization (preserves feature relationships)\n",
        "5. ✅ Cosine annealing (better convergence)\n",
        "6. ✅ Gradient clipping (stability)\n",
        "\n",
        "**Next Steps:**\n",
        "1. Upload `submission.csv` to Kaggle\n",
        "2. Share notebook with TAs\n",
        "3. Use in Part 2"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
