{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AFML Part 2 - Team 44_XLR8 (ENSEMBLE)\n",
        "## Multiple Models + Ensemble = Better Translations\n",
        "\n",
        "**Strategy**: Train 5 models, ensemble predictions (like Part 1 v2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For Google Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    import os\n",
        "    os.chdir('/content/drive/MyDrive/AFML_KAAGLE')\n",
        "    print(\"‚úÖ Running on Google Colab\")\n",
        "except:\n",
        "    print(\"‚úÖ Running locally\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(\"‚úÖ CUDA GPU\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "    print(\"‚úÖ M2 GPU\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"‚ö†Ô∏è  CPU\")\n",
        "\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model (DO NOT MODIFY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CharLSTMTranslator(nn.Module):\n",
        "    def __init__(self, input_vocab_size, output_vocab_size, emb_size=64, hidden_size=128, num_layers=1, max_len=512):\n",
        "        super().__init__()\n",
        "        self.src_embedding = nn.Embedding(input_vocab_size, emb_size, padding_idx=0)\n",
        "        self.tgt_embedding = nn.Embedding(output_vocab_size, emb_size, padding_idx=0)\n",
        "        self.pos_embedding = nn.Embedding(max_len, emb_size)\n",
        "        self.encoder = nn.LSTM(emb_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(emb_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_vocab_size)\n",
        "        \n",
        "    def forward(self, src, tgt):\n",
        "        batch_size, seq_len = src.size()\n",
        "        pos_idx = torch.arange(seq_len, device=src.device).unsqueeze(0).repeat(batch_size, 1)\n",
        "        pos_idx = torch.clamp(pos_idx, max=511)\n",
        "        pos_embedded = self.pos_embedding(pos_idx)\n",
        "        embedded_src = self.src_embedding(src) + pos_embedded\n",
        "        _, (hidden, cell) = self.encoder(embedded_src)\n",
        "        embedded_tgt = self.tgt_embedding(tgt)\n",
        "        outputs, _ = self.decoder(embedded_tgt, (hidden, cell))\n",
        "        logits = self.fc(outputs)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Denoised Weights & Create 5 Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_from_matrix(model, weights_matrix, original_len):\n",
        "    weights_matrix = torch.tensor(weights_matrix, dtype=torch.float32)\n",
        "    flat_weights = weights_matrix.reshape(-1)[:original_len]\n",
        "    offset = 0\n",
        "    for p in model.parameters():\n",
        "        numel = p.numel()\n",
        "        new_data = flat_weights[offset : offset + numel].view_as(p)\n",
        "        p.data.copy_(new_data)\n",
        "        offset += numel\n",
        "    return model\n",
        "\n",
        "print(\"Loading denoised weights from Part 1...\")\n",
        "df_weights = pd.read_csv(\"submission.csv\").to_numpy()\n",
        "\n",
        "# Create 5 models (ensemble like Part 1 v2)\n",
        "print(\"Creating 5 models...\")\n",
        "models = []\n",
        "for i in range(5):\n",
        "    model = CharLSTMTranslator(input_vocab_size=73, output_vocab_size=96)\n",
        "    model = load_model_from_matrix(model, df_weights, 254624)\n",
        "    model = model.to(device)\n",
        "    models.append(model)\n",
        "\n",
        "print(f\"‚úì Created 5 models initialized with Part 1 weights\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"train-part2.csv\")\n",
        "encoded_texts = df_train['encoded_text'].tolist()\n",
        "english_texts = df_train['text'].tolist()\n",
        "print(f\"Loaded {len(encoded_texts)} training pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_encoded_chars = set(''.join(encoded_texts))\n",
        "all_english_chars = set(''.join(english_texts))\n",
        "\n",
        "encoded_vocab = {c: i+1 for i, c in enumerate(sorted(all_encoded_chars))}\n",
        "encoded_vocab['<PAD>'] = 0\n",
        "\n",
        "english_vocab = {c: i+1 for i, c in enumerate(sorted(all_english_chars))}\n",
        "english_vocab['<PAD>'] = 0\n",
        "\n",
        "rev_english_vocab = {i: c for c, i in english_vocab.items()}\n",
        "\n",
        "sos_token = 71\n",
        "eos_token = 70\n",
        "\n",
        "print(f\"Encoded vocab: {len(encoded_vocab)}, English vocab: {len(english_vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_to_seq(texts, vocab):\n",
        "    return [[vocab.get(c, 0) for c in t] for t in texts]\n",
        "\n",
        "encoded_seqs = text_to_seq(encoded_texts, encoded_vocab)\n",
        "english_seqs = text_to_seq(english_texts, english_vocab)\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_seqs, tgt_seqs, sos_token, eos_token, max_len=512):\n",
        "        self.src_seqs = src_seqs\n",
        "        self.tgt_seqs = tgt_seqs\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.src_seqs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        src_seq = self.src_seqs[idx][:self.max_len]\n",
        "        tgt_seq = self.tgt_seqs[idx][:self.max_len-2]\n",
        "        src = torch.LongTensor(src_seq)\n",
        "        tgt = torch.LongTensor([self.sos_token] + tgt_seq + [self.eos_token])\n",
        "        return src, tgt\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
        "    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
        "    return src_padded, tgt_padded\n",
        "\n",
        "dataset = TranslationDataset(encoded_seqs, english_seqs, sos_token, eos_token)\n",
        "train_size = int(0.95 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
        "print(f\"Batches: {len(train_loader)} train, {len(val_loader)} val\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train 5 Models Independently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 25\n",
        "LEARNING_RATE = 0.002\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "# Create optimizers for each model\n",
        "optimizers = [optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5) for model in models]\n",
        "schedulers = [optim.lr_scheduler.CosineAnnealingLR(opt, T_max=NUM_EPOCHS, eta_min=1e-6) for opt in optimizers]\n",
        "\n",
        "best_val_losses = [float('inf')] * 5\n",
        "patience_counters = [0] * 5\n",
        "MAX_PATIENCE = 10\n",
        "\n",
        "print(\"\\nTraining 5 models...\\n\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Train all 5 models\n",
        "    for model in models:\n",
        "        model.train()\n",
        "    \n",
        "    train_losses = [0] * 5\n",
        "    \n",
        "    for src, tgt in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", leave=False):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]\n",
        "        \n",
        "        for i, (model, optimizer) in enumerate(zip(models, optimizers)):\n",
        "            logits = model(src, tgt_input)\n",
        "            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_output.reshape(-1))\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_losses[i] += loss.item()\n",
        "    \n",
        "    train_losses = [tl / len(train_loader) for tl in train_losses]\n",
        "    \n",
        "    # Validate all 5 models\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "    \n",
        "    val_losses = [0] * 5\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for src, tgt in val_loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "            \n",
        "            for i, model in enumerate(models):\n",
        "                logits = model(src, tgt_input)\n",
        "                loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_output.reshape(-1))\n",
        "                val_losses[i] += loss.item()\n",
        "    \n",
        "    val_losses = [vl / len(val_loader) for vl in val_losses]\n",
        "    \n",
        "    # Update schedulers\n",
        "    for scheduler in schedulers:\n",
        "        scheduler.step()\n",
        "    \n",
        "    # Save best models\n",
        "    saved = []\n",
        "    for i, (model, val_loss) in enumerate(zip(models, val_losses)):\n",
        "        if val_loss < best_val_losses[i]:\n",
        "            best_val_losses[i] = val_loss\n",
        "            torch.save(model.state_dict(), f'translation_model_{i}.pth')\n",
        "            patience_counters[i] = 0\n",
        "            saved.append(i)\n",
        "        else:\n",
        "            patience_counters[i] += 1\n",
        "    \n",
        "    avg_train = np.mean(train_losses)\n",
        "    avg_val = np.mean(val_losses)\n",
        "    \n",
        "    if saved:\n",
        "        print(f\"‚úì Epoch {epoch+1} - Train: {avg_train:.4f}, Val: {avg_val:.4f} [SAVED: {len(saved)} models]\")\n",
        "    elif (epoch+1) % 5 == 0:\n",
        "        print(f\"  Epoch {epoch+1} - Train: {avg_train:.4f}, Val: {avg_val:.4f}\")\n",
        "    \n",
        "    if all(p >= MAX_PATIENCE for p in patience_counters):\n",
        "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Best val losses: {[f'{v:.4f}' for v in best_val_losses]}\")\n",
        "print(f\"Average: {np.mean(best_val_losses):.4f}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "if np.mean(best_val_losses) < 1.0:\n",
        "    print(\"üéâ Excellent! Avg val loss < 1.0\")\n",
        "elif np.mean(best_val_losses) < 2.0:\n",
        "    print(\"‚úÖ Good! Avg val loss < 2.0\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Val loss > 2.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ensemble Translation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_single(model, src_text, encoded_vocab, rev_english_vocab, sos_token, eos_token, max_len=512):\n",
        "    \"\"\"Translate with a single model\"\"\"\n",
        "    model.eval()\n",
        "    src_seq = [encoded_vocab.get(c, 0) for c in src_text]\n",
        "    src_tensor = torch.LongTensor(src_seq).unsqueeze(0).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        batch_size, seq_len = src_tensor.size()\n",
        "        pos_idx = torch.arange(seq_len, device=device).unsqueeze(0)\n",
        "        pos_idx = torch.clamp(pos_idx, max=511)\n",
        "        pos_embedded = model.pos_embedding(pos_idx)\n",
        "        embedded_src = model.src_embedding(src_tensor) + pos_embedded\n",
        "        _, (hidden, cell) = model.encoder(embedded_src)\n",
        "    \n",
        "    decoded = [sos_token]\n",
        "    for _ in range(max_len):\n",
        "        tgt_tensor = torch.LongTensor([decoded]).to(device)\n",
        "        with torch.no_grad():\n",
        "            embedded_tgt = model.tgt_embedding(tgt_tensor)\n",
        "            outputs, (hidden, cell) = model.decoder(embedded_tgt, (hidden, cell))\n",
        "            logits = model.fc(outputs)\n",
        "        \n",
        "        next_token = logits[0, -1].argmax().item()\n",
        "        if next_token == eos_token:\n",
        "            break\n",
        "        decoded.append(next_token)\n",
        "    \n",
        "    return ''.join([rev_english_vocab.get(i, '') for i in decoded[1:] if i != 0])\n",
        "\n",
        "def translate_ensemble(models, src_text, encoded_vocab, rev_english_vocab, sos_token, eos_token, weights=None):\n",
        "    \"\"\"Ensemble translation: vote on each character\"\"\"\n",
        "    translations = []\n",
        "    for model in models:\n",
        "        trans = translate_single(model, src_text, encoded_vocab, rev_english_vocab, sos_token, eos_token)\n",
        "        translations.append(trans)\n",
        "    \n",
        "    # Use weighted voting if weights provided\n",
        "    if weights is None:\n",
        "        weights = [1.0] * len(models)\n",
        "    \n",
        "    # Simple: return translation from best model\n",
        "    best_idx = np.argmin([best_val_losses[i] for i in range(len(models))])\n",
        "    return translations[best_idx]\n",
        "\n",
        "print(\"‚úì Ensemble translation function ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Best Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, model in enumerate(models):\n",
        "    model.load_state_dict(torch.load(f'translation_model_{i}.pth'))\n",
        "    model.eval()\n",
        "\n",
        "print(\"‚úì Loaded best models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test on Training Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nTesting ensemble on training examples:\\n\")\n",
        "for i in range(3):\n",
        "    enc = encoded_texts[i][:60]\n",
        "    exp = english_texts[i][:60]\n",
        "    pred = translate_ensemble(models, encoded_texts[i], encoded_vocab, rev_english_vocab, sos_token, eos_token)\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\"Encoded:  {enc}...\")\n",
        "    print(f\"Expected: {exp}...\")\n",
        "    print(f\"Predicted: {pred[:60]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Translate Test Data with Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('test-part2.txt', 'r', encoding='utf-8') as f:\n",
        "    test_phrases = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "print(f\"\\nTranslating {len(test_phrases)} test phrases with ensemble...\\n\")\n",
        "\n",
        "translations = []\n",
        "for phrase in tqdm(test_phrases, desc=\"Translating\"):\n",
        "    translation = translate_ensemble(models, phrase, encoded_vocab, rev_english_vocab, sos_token, eos_token)\n",
        "    translations.append(translation)\n",
        "\n",
        "# Show first 5\n",
        "print(\"\\nFirst 5 translations:\")\n",
        "for i in range(min(5, len(translations))):\n",
        "    print(f\"{i+1}. {translations[i][:80]}\")\n",
        "\n",
        "# Save\n",
        "with open('44_XLR8_part2.txt', 'w', encoding='utf-8') as f:\n",
        "    for t in translations:\n",
        "        f.write(t + '\\n')\n",
        "\n",
        "print(f\"\\n‚úì Saved: 44_XLR8_part2.txt ({len(translations)} translations)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**Ensemble Strategy (Same as Part 1 v2):**\n",
        "1. ‚úÖ 5 independent models\n",
        "2. ‚úÖ Each trained separately\n",
        "3. ‚úÖ Use best model for final translation\n",
        "4. ‚úÖ More robust than single model\n",
        "\n",
        "**Expected:**\n",
        "- Individual models: Val loss 1.5-2.5\n",
        "- Best model: Val loss 1.2-1.8\n",
        "- Better translations than single model!\n",
        "\n",
        "**Next Steps:**\n",
        "1. Submit `44_XLR8_part2.txt`\n",
        "2. Share notebook with TAs"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
