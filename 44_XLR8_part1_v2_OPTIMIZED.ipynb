{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AFML Part 1 - Team 44_XLR8 (v2 OPTIMIZED)\n",
        "## Target: Val Loss < 0.005 and NMSE < 0.3\n",
        "\n",
        "**Strategy**: NO normalization + Ensemble + Deeper network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "    print(\"✅ M2 GPU\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(\"✅ CUDA GPU\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"⚠️  CPU\")\n",
        "\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading...\")\n",
        "train_clean = pd.read_csv('train-part1-clean.csv').values.astype(np.float32)\n",
        "train_noise = pd.read_csv('train-part1-noise.csv').values.astype(np.float32)\n",
        "test_data = pd.read_csv('test-part1.csv').values.astype(np.float32)\n",
        "\n",
        "print(f\"Clean: {train_clean.shape}\")\n",
        "print(f\"Noisy: {train_noise.shape}\")\n",
        "print(f\"Test: {test_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NO NORMALIZATION - Direct Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split only - NO NORMALIZATION\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    train_noise, train_clean, test_size=0.15, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}\")\n",
        "\n",
        "# Direct to tensors\n",
        "X_train_t = torch.FloatTensor(X_train)\n",
        "y_train_t = torch.FloatTensor(y_train)\n",
        "X_val_t = torch.FloatTensor(X_val)\n",
        "y_val_t = torch.FloatTensor(y_val)\n",
        "test_t = torch.FloatTensor(test_data)\n",
        "\n",
        "print(\"✓ No normalization - raw data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deeper Model with More Capacity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DeepDenoiser(nn.Module):\n",
        "    def __init__(self, input_dim=20):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            \n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            \n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            \n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            \n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            \n",
        "            nn.Linear(256, input_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "print(\"Creating ensemble of 5 models...\")\n",
        "models = [DeepDenoiser().to(device) for _ in range(5)]\n",
        "print(f\"Params per model: {sum(p.numel() for p in models[0].parameters()):,}\")\n",
        "print(f\"Total params: {sum(p.numel() for p in models[0].parameters()) * 5:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Setup - Aggressive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 4096  # Larger batches\n",
        "NUM_EPOCHS = 120   # More epochs\n",
        "LR = 0.0003        # Lower LR for stability\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    TensorDataset(X_train_t, y_train_t),\n",
        "    batch_size=BATCH_SIZE, shuffle=True, drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    TensorDataset(X_val_t, y_val_t),\n",
        "    batch_size=BATCH_SIZE, shuffle=False\n",
        ")\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizers = [optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-6) for model in models]\n",
        "schedulers = [optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=8) for opt in optimizers]\n",
        "\n",
        "print(f\"Batches/epoch: {len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_val_losses = [float('inf')] * 5\n",
        "patience_counters = [0] * 5\n",
        "MAX_PATIENCE = 25\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Train\n",
        "    for model in models:\n",
        "        model.train()\n",
        "    \n",
        "    train_losses = [0] * 5\n",
        "    \n",
        "    for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        \n",
        "        for i, (model, optimizer) in enumerate(zip(models, optimizers)):\n",
        "            pred = model(X_batch)\n",
        "            loss = criterion(pred, y_batch)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_losses[i] += loss.item()\n",
        "    \n",
        "    train_losses = [tl / len(train_loader) for tl in train_losses]\n",
        "    \n",
        "    # Validate\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "    \n",
        "    val_losses = [0] * 5\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            \n",
        "            for i, model in enumerate(models):\n",
        "                pred = model(X_batch)\n",
        "                loss = criterion(pred, y_batch)\n",
        "                val_losses[i] += loss.item()\n",
        "    \n",
        "    val_losses = [vl / len(val_loader) for vl in val_losses]\n",
        "    \n",
        "    # Update schedulers\n",
        "    for i, (scheduler, val_loss) in enumerate(zip(schedulers, val_losses)):\n",
        "        scheduler.step(val_loss)\n",
        "    \n",
        "    # Save best\n",
        "    saved = []\n",
        "    for i, (model, val_loss) in enumerate(zip(models, val_losses)):\n",
        "        if val_loss < best_val_losses[i]:\n",
        "            best_val_losses[i] = val_loss\n",
        "            torch.save(model.state_dict(), f'model_{i}.pth')\n",
        "            patience_counters[i] = 0\n",
        "            saved.append(i)\n",
        "        else:\n",
        "            patience_counters[i] += 1\n",
        "    \n",
        "    avg_train = np.mean(train_losses)\n",
        "    avg_val = np.mean(val_losses)\n",
        "    \n",
        "    if saved:\n",
        "        print(f\"✓ Epoch {epoch+1} - Train: {avg_train:.6f}, Val: {avg_val:.6f} [SAVED: {saved}]\")\n",
        "    elif (epoch+1) % 10 == 0:\n",
        "        print(f\"  Epoch {epoch+1} - Train: {avg_train:.6f}, Val: {avg_val:.6f}\")\n",
        "    \n",
        "    # Early stopping if all models stopped improving\n",
        "    if all(p >= MAX_PATIENCE for p in patience_counters):\n",
        "        print(f\"\\nEarly stop at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nBest val losses: {[f'{v:.6f}' for v in best_val_losses]}\")\n",
        "print(f\"Average: {np.mean(best_val_losses):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ensemble Prediction with TTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best models\n",
        "for i, model in enumerate(models):\n",
        "    model.load_state_dict(torch.load(f'model_{i}.pth'))\n",
        "    model.eval()\n",
        "\n",
        "print(\"Predicting with ensemble + TTA...\")\n",
        "\n",
        "def predict_ensemble_tta(models, data_tensor, n_tta=3):\n",
        "    \"\"\"Ensemble + Test Time Augmentation\"\"\"\n",
        "    all_preds = []\n",
        "    data_tensor = data_tensor.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for model in models:\n",
        "            # Original\n",
        "            pred = model(data_tensor).cpu().numpy()\n",
        "            all_preds.append(pred)\n",
        "            \n",
        "            # TTA with small noise\n",
        "            for _ in range(n_tta - 1):\n",
        "                noise = torch.randn_like(data_tensor) * 0.005\n",
        "                pred = model(data_tensor + noise).cpu().numpy()\n",
        "                all_preds.append(pred)\n",
        "    \n",
        "    return np.mean(all_preds, axis=0)\n",
        "\n",
        "test_pred = predict_ensemble_tta(models, test_t, n_tta=3)\n",
        "val_pred = predict_ensemble_tta(models, X_val_t, n_tta=3)\n",
        "\n",
        "print(\"✓ Ensemble + TTA done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate NMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mse = np.mean((y_val - val_pred) ** 2)\n",
        "variance = np.var(y_val)\n",
        "nmse = mse / variance\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"VALIDATION NMSE: {nmse:.6f}\")\n",
        "print(f\"Target: < 0.3\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "if nmse < 0.3:\n",
        "    print(f\"🎉 SUCCESS! NMSE < 0.3!\")\n",
        "elif nmse < 0.5:\n",
        "    print(f\"⚠️  Close! NMSE = {nmse:.4f}\")\n",
        "else:\n",
        "    print(f\"❌ NMSE too high: {nmse:.4f}\")\n",
        "\n",
        "print(f\"\\nMSE: {mse:.8f}\")\n",
        "print(f\"Variance: {variance:.8f}\")\n",
        "print(f\"Avg Val Loss: {np.mean(best_val_losses):.6f}\")\n",
        "print(f\"Best individual: {min(best_val_losses):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission = pd.DataFrame(test_pred)\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"\\n✓ Saved: submission.csv\")\n",
        "print(f\"Shape: {submission.shape}\")\n",
        "submission.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "for i in range(20):\n",
        "    plt.scatter(y_val[:500, i], val_pred[:500, i], alpha=0.5, s=2)\n",
        "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title(f'NMSE={nmse:.4f}')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "errors = np.abs(y_val - val_pred)\n",
        "plt.hist(errors.flatten(), bins=100, alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Absolute Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Error Distribution')\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal NMSE: {nmse:.6f}\")\n",
        "print(f\"MAE: {np.mean(errors):.6f}\")\n",
        "print(f\"Median AE: {np.median(errors):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**v2 OPTIMIZED Improvements:**\n",
        "1. ✅ NO normalization (like v1 - fast learning)\n",
        "2. ✅ 5 models ensemble (more averaging)\n",
        "3. ✅ Deeper network (1024 hidden units)\n",
        "4. ✅ Lower dropout (0.05 vs 0.1)\n",
        "5. ✅ Larger batches (4096)\n",
        "6. ✅ More epochs (120)\n",
        "7. ✅ TTA (3x per model = 15 predictions averaged)\n",
        "8. ✅ ReduceLROnPlateau scheduler\n",
        "\n",
        "**Expected:**\n",
        "- Val Loss: 0.003-0.005 (like v1)\n",
        "- NMSE: 0.20-0.30 (better than v1's 0.66 due to ensemble)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
