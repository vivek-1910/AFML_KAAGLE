{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AFML Part 1 - Team 44_XLR8 (v2 OPTIMIZED)\n",
        "## Target: Val Loss < 0.005 and NMSE < 0.3\n",
        "\n",
        "**Strategy**: NO normalization + Ensemble + Deeper network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "    print(\"âœ… M2 GPU\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(\"âœ… CUDA GPU\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"âš ï¸  CPU\")\n",
        "\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading...\")\n",
        "train_clean = pd.read_csv('train-part1-clean.csv').values.astype(np.float32)\n",
        "train_noise = pd.read_csv('train-part1-noise.csv').values.astype(np.float32)\n",
        "test_data = pd.read_csv('test-part1.csv').values.astype(np.float32)\n",
        "\n",
        "print(f\"Clean: {train_clean.shape}\")\n",
        "print(f\"Noisy: {train_noise.shape}\")\n",
        "print(f\"Test: {test_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NO NORMALIZATION - Direct Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split only - NO NORMALIZATION\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    train_noise, train_clean, test_size=0.15, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}\")\n",
        "\n",
        "# Direct to tensors\n",
        "X_train_t = torch.FloatTensor(X_train)\n",
        "y_train_t = torch.FloatTensor(y_train)\n",
        "X_val_t = torch.FloatTensor(X_val)\n",
        "y_val_t = torch.FloatTensor(y_val)\n",
        "test_t = torch.FloatTensor(test_data)\n",
        "\n",
        "print(\"âœ“ No normalization - raw data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deeper Model with More Capacity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DeepDenoiser(nn.Module):\n",
        "    def __init__(self, input_dim=20):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            \n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            \n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            \n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            \n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            \n",
        "            nn.Linear(256, input_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "print(\"Creating ensemble of 5 models...\")\n",
        "models = [DeepDenoiser().to(device) for _ in range(5)]\n",
        "print(f\"Params per model: {sum(p.numel() for p in models[0].parameters()):,}\")\n",
        "print(f\"Total params: {sum(p.numel() for p in models[0].parameters()) * 5:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Setup - Aggressive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 4096  # Larger batches\n",
        "NUM_EPOCHS = 120   # More epochs\n",
        "LR = 0.0003        # Lower LR for stability\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    TensorDataset(X_train_t, y_train_t),\n",
        "    batch_size=BATCH_SIZE, shuffle=True, drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    TensorDataset(X_val_t, y_val_t),\n",
        "    batch_size=BATCH_SIZE, shuffle=False\n",
        ")\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizers = [optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-6) for model in models]\n",
        "schedulers = [optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=8) for opt in optimizers]\n",
        "\n",
        "print(f\"Batches/epoch: {len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_val_losses = [float('inf')] * 5\n",
        "patience_counters = [0] * 5\n",
        "MAX_PATIENCE = 25\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Train\n",
        "    for model in models:\n",
        "        model.train()\n",
        "    \n",
        "    train_losses = [0] * 5\n",
        "    \n",
        "    for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        \n",
        "        for i, (model, optimizer) in enumerate(zip(models, optimizers)):\n",
        "            pred = model(X_batch)\n",
        "            loss = criterion(pred, y_batch)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_losses[i] += loss.item()\n",
        "    \n",
        "    train_losses = [tl / len(train_loader) for tl in train_losses]\n",
        "    \n",
        "    # Validate\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "    \n",
        "    val_losses = [0] * 5\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            \n",
        "            for i, model in enumerate(models):\n",
        "                pred = model(X_batch)\n",
        "                loss = criterion(pred, y_batch)\n",
        "                val_losses[i] += loss.item()\n",
        "    \n",
        "    val_losses = [vl / len(val_loader) for vl in val_losses]\n",
        "    \n",
        "    # Update schedulers\n",
        "    for i, (scheduler, val_loss) in enumerate(zip(schedulers, val_losses)):\n",
        "        scheduler.step(val_loss)\n",
        "    \n",
        "    # Save best\n",
        "    saved = []\n",
        "    for i, (model, val_loss) in enumerate(zip(models, val_losses)):\n",
        "        if val_loss < best_val_losses[i]:\n",
        "            best_val_losses[i] = val_loss\n",
        "            torch.save(model.state_dict(), f'model_{i}.pth')\n",
        "            patience_counters[i] = 0\n",
        "            saved.append(i)\n",
        "        else:\n",
        "            patience_counters[i] += 1\n",
        "    \n",
        "    avg_train = np.mean(train_losses)\n",
        "    avg_val = np.mean(val_losses)\n",
        "    \n",
        "    if saved:\n",
        "        print(f\"âœ“ Epoch {epoch+1} - Train: {avg_train:.6f}, Val: {avg_val:.6f} [SAVED: {saved}]\")\n",
        "    elif (epoch+1) % 10 == 0:\n",
        "        print(f\"  Epoch {epoch+1} - Train: {avg_train:.6f}, Val: {avg_val:.6f}\")\n",
        "    \n",
        "    # Early stopping if all models stopped improving\n",
        "    if all(p >= MAX_PATIENCE for p in patience_counters):\n",
        "        print(f\"\\nEarly stop at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nBest val losses: {[f'{v:.6f}' for v in best_val_losses]}\")\n",
        "print(f\"Average: {np.mean(best_val_losses):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ensemble Prediction with TTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best models\n",
        "for i, model in enumerate(models):\n",
        "    model.load_state_dict(torch.load(f'model_{i}.pth'))\n",
        "    model.eval()\n",
        "\n",
        "print(\"Predicting with ensemble + TTA...\")\n",
        "\n",
        "def predict_ensemble_tta(models, data_tensor, n_tta=3):\n",
        "    \"\"\"Ensemble + Test Time Augmentation\"\"\"\n",
        "    all_preds = []\n",
        "    data_tensor = data_tensor.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for model in models:\n",
        "            # Original\n",
        "            pred = model(data_tensor).cpu().numpy()\n",
        "            all_preds.append(pred)\n",
        "            \n",
        "            # TTA with small noise\n",
        "            for _ in range(n_tta - 1):\n",
        "                noise = torch.randn_like(data_tensor) * 0.005\n",
        "                pred = model(data_tensor + noise).cpu().numpy()\n",
        "                all_preds.append(pred)\n",
        "    \n",
        "    return np.mean(all_preds, axis=0)\n",
        "\n",
        "test_pred = predict_ensemble_tta(models, test_t, n_tta=3)\n",
        "val_pred = predict_ensemble_tta(models, X_val_t, n_tta=3)\n",
        "\n",
        "print(\"âœ“ Ensemble + TTA done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate NMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mse = np.mean((y_val - val_pred) ** 2)\n",
        "variance = np.var(y_val)\n",
        "nmse = mse / variance\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"VALIDATION NMSE: {nmse:.6f}\")\n",
        "print(f\"Target: < 0.3\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "if nmse < 0.3:\n",
        "    print(f\"ðŸŽ‰ SUCCESS! NMSE < 0.3!\")\n",
        "elif nmse < 0.5:\n",
        "    print(f\"âš ï¸  Close! NMSE = {nmse:.4f}\")\n",
        "else:\n",
        "    print(f\"âŒ NMSE too high: {nmse:.4f}\")\n",
        "\n",
        "print(f\"\\nMSE: {mse:.8f}\")\n",
        "print(f\"Variance: {variance:.8f}\")\n",
        "print(f\"Avg Val Loss: {np.mean(best_val_losses):.6f}\")\n",
        "print(f\"Best individual: {min(best_val_losses):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission = pd.DataFrame(test_pred)\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"\\nâœ“ Saved: submission.csv\")\n",
        "print(f\"Shape: {submission.shape}\")\n",
        "submission.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "for i in range(20):\n",
        "    plt.scatter(y_val[:500, i], val_pred[:500, i], alpha=0.5, s=2)\n",
        "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title(f'NMSE={nmse:.4f}')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "errors = np.abs(y_val - val_pred)\n",
        "plt.hist(errors.flatten(), bins=100, alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Absolute Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Error Distribution')\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal NMSE: {nmse:.6f}\")\n",
        "print(f\"MAE: {np.mean(errors):.6f}\")\n",
        "print(f\"Median AE: {np.median(errors):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**v2 OPTIMIZED Improvements:**\n",
        "1. âœ… NO normalization (like v1 - fast learning)\n",
        "2. âœ… 5 models ensemble (more averaging)\n",
        "3. âœ… Deeper network (1024 hidden units)\n",
        "4. âœ… Lower dropout (0.05 vs 0.1)\n",
        "5. âœ… Larger batches (4096)\n",
        "6. âœ… More epochs (120)\n",
        "7. âœ… TTA (3x per model = 15 predictions averaged)\n",
        "8. âœ… ReduceLROnPlateau scheduler\n",
        "\n",
        "**Expected:**\n",
        "- Val Loss: 0.003-0.005 (like v1)\n",
        "- NMSE: 0.20-0.30 (better than v1's 0.66 due to ensemble)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
