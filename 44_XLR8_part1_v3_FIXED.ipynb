{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AFML Part 1 - Team 44_XLR8 (v3 FIXED)\n",
        "## Simple & Effective: Large Ensemble + Data Augmentation\n",
        "\n",
        "**Strategy**: 10 simple models + augmentation = lower NMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "    print(\"‚úÖ M2 GPU\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(\"‚úÖ CUDA GPU\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"‚ö†Ô∏è  CPU\")\n",
        "\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading...\")\n",
        "train_clean = pd.read_csv('train-part1-clean.csv').values.astype(np.float32)\n",
        "train_noise = pd.read_csv('train-part1-noise.csv').values.astype(np.float32)\n",
        "test_data = pd.read_csv('test-part1.csv').values.astype(np.float32)\n",
        "\n",
        "print(f\"Clean: {train_clean.shape}\")\n",
        "print(f\"Noisy: {train_noise.shape}\")\n",
        "print(f\"Test: {test_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    train_noise, train_clean, test_size=0.10, random_state=42  # Less validation = more training\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}\")\n",
        "\n",
        "# Augment training data by adding small noise\n",
        "print(\"Augmenting training data...\")\n",
        "X_train_aug = []\n",
        "y_train_aug = []\n",
        "\n",
        "# Original\n",
        "X_train_aug.append(X_train)\n",
        "y_train_aug.append(y_train)\n",
        "\n",
        "# Add 2 augmented versions\n",
        "for i in range(2):\n",
        "    noise_factor = 0.001 * (i + 1)\n",
        "    X_aug = X_train + np.random.randn(*X_train.shape).astype(np.float32) * noise_factor\n",
        "    X_train_aug.append(X_aug)\n",
        "    y_train_aug.append(y_train)\n",
        "\n",
        "X_train = np.vstack(X_train_aug)\n",
        "y_train = np.vstack(y_train_aug)\n",
        "\n",
        "print(f\"Augmented train: {X_train.shape}\")\n",
        "\n",
        "# To tensors - NO NORMALIZATION\n",
        "X_train_t = torch.FloatTensor(X_train)\n",
        "y_train_t = torch.FloatTensor(y_train)\n",
        "X_val_t = torch.FloatTensor(X_val)\n",
        "y_val_t = torch.FloatTensor(y_val)\n",
        "test_t = torch.FloatTensor(test_data)\n",
        "\n",
        "print(\"‚úì Data ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple but Effective Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_dim=20):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.03),\n",
        "            \n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.03),\n",
        "            \n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.03),\n",
        "            \n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.03),\n",
        "            \n",
        "            nn.Linear(512, input_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "print(\"Creating 10 models...\")\n",
        "models = [SimpleModel().to(device) for _ in range(10)]\n",
        "print(f\"Params per model: {sum(p.numel() for p in models[0].parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16384  # Even larger\n",
        "NUM_EPOCHS = 120\n",
        "LR = 0.0003\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    TensorDataset(X_train_t, y_train_t),\n",
        "    batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    TensorDataset(X_val_t, y_val_t),\n",
        "    batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
        ")\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizers = [optim.Adam(model.parameters(), lr=LR) for model in models]\n",
        "schedulers = [optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=8, min_lr=1e-6) for opt in optimizers]\n",
        "\n",
        "print(f\"Batches/epoch: {len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_val_losses = [float('inf')] * 10\n",
        "patience_counters = [0] * 10\n",
        "MAX_PATIENCE = 25\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Train\n",
        "    for model in models:\n",
        "        model.train()\n",
        "    \n",
        "    train_losses = [0] * 10\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    for X_batch, y_batch in pbar:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        \n",
        "        for i, (model, optimizer) in enumerate(zip(models, optimizers)):\n",
        "            pred = model(X_batch)\n",
        "            loss = criterion(pred, y_batch)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_losses[i] += loss.item()\n",
        "        \n",
        "        pbar.set_postfix({'loss': f'{np.mean(train_losses) / (pbar.n + 1):.6f}'})\n",
        "    \n",
        "    train_losses = [tl / len(train_loader) for tl in train_losses]\n",
        "    \n",
        "    # Validate\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "    \n",
        "    val_losses = [0] * 10\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            \n",
        "            for i, model in enumerate(models):\n",
        "                pred = model(X_batch)\n",
        "                loss = criterion(pred, y_batch)\n",
        "                val_losses[i] += loss.item()\n",
        "    \n",
        "    val_losses = [vl / len(val_loader) for vl in val_losses]\n",
        "    \n",
        "    # Schedulers\n",
        "    for i, (scheduler, val_loss) in enumerate(zip(schedulers, val_losses)):\n",
        "        scheduler.step(val_loss)\n",
        "    \n",
        "    # Save\n",
        "    saved = []\n",
        "    for i, (model, val_loss) in enumerate(zip(models, val_losses)):\n",
        "        if val_loss < best_val_losses[i]:\n",
        "            best_val_losses[i] = val_loss\n",
        "            torch.save(model.state_dict(), f'model_{i}.pth')\n",
        "            patience_counters[i] = 0\n",
        "            saved.append(i)\n",
        "        else:\n",
        "            patience_counters[i] += 1\n",
        "    \n",
        "    avg_train = np.mean(train_losses)\n",
        "    avg_val = np.mean(val_losses)\n",
        "    \n",
        "    if saved:\n",
        "        print(f\"‚úì Epoch {epoch+1} - Train: {avg_train:.6f}, Val: {avg_val:.6f} [SAVED: {len(saved)} models]\")\n",
        "    elif (epoch+1) % 10 == 0:\n",
        "        print(f\"  Epoch {epoch+1} - Train: {avg_train:.6f}, Val: {avg_val:.6f}\")\n",
        "    \n",
        "    if all(p >= MAX_PATIENCE for p in patience_counters):\n",
        "        print(f\"\\nEarly stop at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nBest val losses: {[f'{v:.6f}' for v in best_val_losses]}\")\n",
        "print(f\"Average: {np.mean(best_val_losses):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predict with Weighted Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, model in enumerate(models):\n",
        "    model.load_state_dict(torch.load(f'model_{i}.pth'))\n",
        "    model.eval()\n",
        "\n",
        "print(\"Predicting with weighted ensemble...\")\n",
        "\n",
        "# Weight models by inverse of their validation loss\n",
        "weights = 1.0 / np.array(best_val_losses)\n",
        "weights = weights / weights.sum()  # Normalize\n",
        "\n",
        "print(f\"Model weights: {weights}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_t_device = test_t.to(device)\n",
        "    X_val_t_device = X_val_t.to(device)\n",
        "    \n",
        "    test_preds = [model(test_t_device).cpu().numpy() for model in models]\n",
        "    val_preds = [model(X_val_t_device).cpu().numpy() for model in models]\n",
        "    \n",
        "    # Weighted average\n",
        "    test_pred = sum(w * p for w, p in zip(weights, test_preds))\n",
        "    val_pred = sum(w * p for w, p in zip(weights, val_preds))\n",
        "\n",
        "print(\"‚úì Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mse = np.mean((y_val - val_pred) ** 2)\n",
        "variance = np.var(y_val)\n",
        "nmse = mse / variance\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"VALIDATION NMSE: {nmse:.6f}\")\n",
        "print(f\"Target: < 0.3\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "if nmse < 0.3:\n",
        "    print(f\"üéâ SUCCESS! NMSE < 0.3!\")\n",
        "elif nmse < 0.5:\n",
        "    print(f\"‚ö†Ô∏è  Close! NMSE = {nmse:.4f}\")\n",
        "else:\n",
        "    print(f\"‚ùå NMSE = {nmse:.4f}\")\n",
        "\n",
        "print(f\"\\nMSE: {mse:.8f}\")\n",
        "print(f\"Variance: {variance:.8f}\")\n",
        "print(f\"Avg Val Loss: {np.mean(best_val_losses):.6f}\")\n",
        "print(f\"Best model: {min(best_val_losses):.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission = pd.DataFrame(test_pred)\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"‚úì Saved: submission.csv\")\n",
        "print(f\"Shape: {submission.shape}\")\n",
        "submission.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "for i in range(20):\n",
        "    plt.scatter(y_val[:500, i], val_pred[:500, i], alpha=0.5, s=2)\n",
        "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title(f'NMSE={nmse:.4f}')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "errors = np.abs(y_val - val_pred)\n",
        "plt.hist(errors.flatten(), bins=100, alpha=0.7)\n",
        "plt.xlabel('Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal NMSE: {nmse:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## v3 FIXED Strategy:\n",
        "\n",
        "1. ‚úÖ **10 models** (more diversity than v2's 5)\n",
        "2. ‚úÖ **Data augmentation** (3x training data)\n",
        "3. ‚úÖ **Weighted ensemble** (better models get more weight)\n",
        "4. ‚úÖ **Less validation** (10% vs 15% = more training)\n",
        "5. ‚úÖ **Larger batches** (16384 = faster)\n",
        "6. ‚úÖ **Lower dropout** (0.03 vs 0.05 = less regularization)\n",
        "\n",
        "**Expected: NMSE 0.20-0.28** ‚úÖ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
