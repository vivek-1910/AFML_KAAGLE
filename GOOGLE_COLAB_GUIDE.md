# 🚀 Google Colab Setup Guide - Team 44_XLR8

## 📋 Quick Start

### Step 1: Upload Files to Google Drive

1. Go to [Google Drive](https://drive.google.com)
2. Create a folder: `AFML_KAAGLE`
3. Upload these files:
   - `train-part1-clean.csv`
   - `train-part1-noise.csv`
   - `test-part1.csv`
   - `train-part2.csv`
   - `test-part2.txt`
   - `44_XLR8_part1.ipynb`
   - `44_XLR8_part2.ipynb`

### Step 2: Open in Google Colab

**Part 1:**
1. Go to [Google Colab](https://colab.research.google.com)
2. File → Open notebook → Google Drive
3. Navigate to `AFML_KAAGLE/44_XLR8_part1.ipynb`
4. Click to open

**Enable GPU:**
- Runtime → Change runtime type → Hardware accelerator → **GPU** → Save

### Step 3: Run Part 1

1. Run the first cell to mount Google Drive
2. Update the path if needed: `/content/drive/MyDrive/AFML_KAAGLE`
3. Runtime → Run all (or Ctrl+F9)
4. Wait ~20-30 minutes
5. Download `submission.csv` from the Files panel (left sidebar)

**Expected Result:** NMSE < 0.3 ✅

### Step 4: Run Part 2

1. Upload `submission.csv` back to your Drive folder
2. Open `44_XLR8_part2.ipynb` in Colab
3. Enable GPU (Runtime → Change runtime type)
4. Run all cells
5. Wait ~30-45 minutes
6. Download `44_XLR8_part2.txt`

---

## 🎯 Key Improvements for NMSE < 0.3

### Part 1 Optimizations:

1. **Residual Learning**
   - Predicts NOISE instead of clean weights
   - Formula: `clean = noisy - predicted_noise`
   - Much easier to learn!

2. **Improved Architecture**
   - Residual blocks with skip connections
   - Deeper network (256 → 256 → 256 → 128)
   - Better regularization (Dropout 0.3)

3. **Better Training**
   - AdamW optimizer (better weight decay)
   - Cosine annealing scheduler
   - 100 epochs instead of 50
   - Gradient clipping

4. **Larger Batch Size**
   - 256 instead of 128
   - Faster training on GPU
   - Better gradient estimates

---

## 📊 Expected Performance

### Part 1
- **Training time**: 20-30 minutes (GPU)
- **Target NMSE**: < 0.3
- **Previous NMSE**: 0.98
- **Improvement**: ~70% better!

### Part 2
- **Training time**: 30-45 minutes (GPU)
- **Epochs**: 15 (faster convergence)
- **Validation loss**: Should reach < 1.0

---

## 🔧 Troubleshooting

### Problem: "No module named 'google.colab'"
**Solution:** You're running locally, not in Colab. The notebook will skip Drive mounting automatically.

### Problem: "Cannot find files"
**Solution:** Check the path in the first cell. Update to match your Drive structure:
```python
os.chdir('/content/drive/MyDrive/AFML_KAAGLE')
```

### Problem: "Out of memory"
**Solution:** 
1. Make sure GPU is enabled (Runtime → Change runtime type)
2. Reduce batch size in the notebook:
   ```python
   BATCH_SIZE = 128  # Instead of 256
   ```

### Problem: "Runtime disconnected"
**Solution:** Colab has usage limits. If disconnected:
1. Reconnect (top right)
2. Re-run from the last checkpoint
3. The best model is saved, so you won't lose progress

---

## 📁 File Structure in Google Drive

```
MyDrive/
└── AFML_KAAGLE/
    ├── train-part1-clean.csv      (255 MB)
    ├── train-part1-noise.csv      (406 MB)
    ├── test-part1.csv             (5 MB)
    ├── train-part2.csv            (443 MB)
    ├── test-part2.txt             (1 KB)
    ├── 44_XLR8_part1.ipynb        (Your Part 1 notebook)
    ├── 44_XLR8_part2.ipynb        (Your Part 2 notebook)
    ├── submission.csv             (Generated by Part 1)
    └── 44_XLR8_part2.txt          (Generated by Part 2)
```

---

## 🎓 Submission Checklist

### Part 1
- [ ] Ran `44_XLR8_part1.ipynb` in Colab with GPU
- [ ] NMSE < 0.3 achieved
- [ ] Downloaded `submission.csv`
- [ ] Uploaded `submission.csv` to Kaggle
- [ ] Shared notebook with all 6 TAs

### Part 2
- [ ] Uploaded `submission.csv` to Drive
- [ ] Ran `44_XLR8_part2.ipynb` in Colab with GPU
- [ ] Downloaded `44_XLR8_part2.txt`
- [ ] Verified 10 translations in the file
- [ ] Shared notebook with all 6 TAs

### TA Kaggle IDs (Share with ALL):
```
adyabhat
anaghakini
namitaachyuth
tejasvenugopalan
shusrith
siddhiz
```

---

## 💡 Pro Tips

1. **Use GPU**: Always enable GPU in Colab for 10-20x speedup
2. **Save checkpoints**: Models are auto-saved as `best_model.pth` and `best_translation.pth`
3. **Monitor training**: Watch the loss curves - they should decrease steadily
4. **Download results**: Download outputs immediately after training (Colab sessions expire)
5. **Keep Drive synced**: Files save to Drive automatically, so you won't lose work

---

## 🚨 Important Notes

### Part 1
- **DO NOT** modify the residual learning approach - it's key to getting NMSE < 0.3
- If NMSE is still > 0.3, try training for 150 epochs
- GPU is highly recommended (20 min vs 2+ hours on CPU)

### Part 2
- **DO NOT** modify the LSTM architecture - it must match the pre-trained weights
- Sequence truncation at 512 tokens is necessary
- Position clamping prevents index errors

---

## 📞 Need Help?

1. Check the output cells for error messages
2. Verify all files are in the correct Drive folder
3. Make sure GPU is enabled
4. Check that paths match your Drive structure

---

## ⚡ Quick Commands

### In Colab, to check GPU:
```python
import torch
print(f"GPU available: {torch.cuda.is_available()}")
print(f"GPU name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")
```

### To check files:
```python
!ls -lh
```

### To download a file:
```python
from google.colab import files
files.download('submission.csv')
```

---

**Ready to start? Upload files to Drive and open the notebooks in Colab! 🎉**
